{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c0bc29-c79a-4d2e-be89-ba1eb4069390",
   "metadata": {},
   "source": [
    "# Earthquake Damage in Sindhupalchok, Nepal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c1ae5-4c44-4eed-82c7-1ca87804c822",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, I will develop a binary classification model to identify buildings in Sindhupalchok, Nepal, as either damaged or not damaged from the 2015 earthquake. This model not only categorizes the buildings but also delves into the underlying decision-making factors, using metrics like Gini importance, to provide insights into what influences damage severity. This comprehensive approach aims to enhance post-disaster analysis and aid in effective rebuilding strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848ad0a-f59f-43ea-82d3-042871bb2f76",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2e61e5-c432-4e8f-9b4e-0f558f4add0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "# Libraries for Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Library for DataFrame creation and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Library for model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for feature transformation\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Libraries for model development\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Library for model performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Library for interactive widget\n",
    "from ipywidgets import interact, Dropdown, FloatSlider, IntSlider\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d6679-1d01-4003-8f2c-a7a4dcc4184a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf73dfc8-3888-4f9e-868e-c0d26d00fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "\n",
    "    # Query to form tablr from database\n",
    "    query = '''\n",
    "    SELECT \n",
    "        DISTINCT (m.building_id) as b_id,\n",
    "        bs.*\n",
    "    FROM mapping m\n",
    "    JOIN \n",
    "        building_structure bs ON m.building_id = bs.building_id\n",
    "    WHERE bs.district_id = 23\n",
    "    '''\n",
    "    connection = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql(sql=query, con=connection, index_col='b_id')\n",
    "    connection.close()\n",
    "\n",
    "    # Create list of columns to drop\n",
    "    drop = list()\n",
    "\n",
    "    # Get a list of all the columns\n",
    "    columns = list(df.columns)\n",
    "\n",
    "    # Add the duplicate `building_id` column to the drop list\n",
    "    drop.append('building_id')\n",
    "\n",
    "    # Extract a column for binary classification\n",
    "    df['severe_damage'] = df['damage_grade'].str.split().str[1].astype('int').apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "    # Remove the `damage_grade` column by adding it to the drop list\n",
    "    drop.append('damage_grade')\n",
    "\n",
    "    # Remove other `id` columns by adding them to the drop list\n",
    "    id_cols = [col for col in columns if '_id' in col]\n",
    "    drop.extend(id_cols)\n",
    "\n",
    "    # Remove columns that can cause leakage by adding them to the drop list\n",
    "    \n",
    "    leakages = [col for col in columns if 'post_eq' in col]\n",
    "    leakages.append('technical_solution_proposed')\n",
    "    drop.extend(leakages)\n",
    "    \n",
    "    # List of encoded columns\n",
    "    one_hot_columns = [col for col in columns if 'has_superstructure' in col]\n",
    "\n",
    "    # Function to find the original categories from one-hot encoded columns and undo encoding\n",
    "    def find_categories(row):\n",
    "        # Create a list for all the available categories in a row\n",
    "        categories = []\n",
    "        \n",
    "        # Loop through the list of encoded columns\n",
    "        for col in one_hot_columns:\n",
    "            if row[col] == 1:\n",
    "                # Change to the original column name\n",
    "                categories.append(col.split('_', 2)[-1])  \n",
    "        \n",
    "        # If no category is found\n",
    "        if not categories:  \n",
    "            # Return placeholder\n",
    "            return 'Other' \n",
    "        # Join multiple categories with a backslash\n",
    "        return '/'.join(categories)  \n",
    "    \n",
    "    # Apply the function to each row\n",
    "    df['superstructure'] = df.apply(find_categories, axis=1)\n",
    "\n",
    "    # Add the encoded columns to the drop list\n",
    "    df.drop(columns=one_hot_columns, inplace=True)\n",
    "\n",
    "    # Remove columns that can cause multicollinearity by adding them to the drop list\n",
    "    drop.append('count_floors_pre_eq')\n",
    "    \n",
    "\n",
    "    # Drop the columns\n",
    "    df.drop(columns=drop, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f400ca32-6465-4269-8d45-9638af050870",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n    SELECT \n        DISTINCT (m.building_id) as b_id,\n        bs.*\n    FROM mapping m\n    JOIN \n        building_structure bs ON m.building_id = bs.building_id\n    WHERE bs.district_id = 23\n    ': no such table: mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\classification_model\\Lib\\site-packages\\pandas\\io\\sql.py:2262\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2262\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: mapping",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m wrangle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnepal_earthquake.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mwrangle\u001b[1;34m(db_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    DISTINCT (m.building_id) as b_id,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mWHERE bs.district_id = 23\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     13\u001b[0m connection \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(db_path)\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(sql\u001b[38;5;241m=\u001b[39mquery, con\u001b[38;5;241m=\u001b[39mconnection, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m connection\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create list of columns to drop\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\classification_model\\Lib\\site-packages\\pandas\\io\\sql.py:654\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    655\u001b[0m             sql,\n\u001b[0;32m    656\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    657\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    658\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    659\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    660\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    661\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    662\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    663\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\classification_model\\Lib\\site-packages\\pandas\\io\\sql.py:2326\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2317\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2324\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2326\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2327\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\classification_model\\Lib\\site-packages\\pandas\\io\\sql.py:2274\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2274\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\n    SELECT \n        DISTINCT (m.building_id) as b_id,\n        bs.*\n    FROM mapping m\n    JOIN \n        building_structure bs ON m.building_id = bs.building_id\n    WHERE bs.district_id = 23\n    ': no such table: mapping"
     ]
    }
   ],
   "source": [
    "df = wrangle('nepal_earthquake.db')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b2476-617e-4589-95fd-b6fd4b35eda8",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4ee7d-6fdb-4d87-ac08-ae0f1311f9f6",
   "metadata": {},
   "source": [
    "To gather insight on possible relationships that exist between the features and the target, I'll perform exploratory data analysis usig suitable plots."
   ]
  },
  {
   "cell_type": "raw",
   "id": "923cb18b-80e5-4660-91ab-c78716e571bf",
   "metadata": {},
   "source": [
    "To get insight on the proportion of building damage in Sindhupalchok, I'll plot a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d9df9-11f1-49bd-adcd-291a5d9ca9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart showing proportion of the binary classes in `severe_damage`\n",
    "df['severe_damage'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.title('Class Balance of Damage in Sindhupalchok');"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1d779a8-0186-4ece-b223-04b8f8504ffc",
   "metadata": {},
   "source": [
    "To find out the relationship between plinth_area_sq_ft and the severe_damage column, we use a box-and-wisker plot from the seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259972c-088c-465b-8b6a-94b72716fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot showing relationship between `plinth_area_sq_ft` and `severe_damage`\n",
    "sns.boxplot(df, x='severe_damage', y='plinth_area_sq_ft')\n",
    "plt.title('Plinth Area vs Building Damage in Sindhupalchok');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975da226-4357-4be1-bd88-340f91a2e26b",
   "metadata": {},
   "source": [
    "To find out if there is materials used for building construction can lead to it being affected by the earthquake, I'll use `pandas` in-built function, `pivot_table` to find the mean of `severe_damage` for each concerned column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41fad4-069a-4c47-aa59-ab2d99d73b30",
   "metadata": {},
   "source": [
    "#### Roof type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e999c-3de9-4b4c-bfe9-688d29a73381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the relationship between \"roof_type\" and \"severe_damage\"\n",
    "pd.pivot_table(df, index='roof_type', \n",
    "               values='severe_damage', aggfunc='mean', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc335ed4-4f2b-4149-8a40-8bc99c550f7d",
   "metadata": {},
   "source": [
    "#### Ground floor type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9115abb-0670-4ed8-ba4f-405400ff15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the relationship between \"ground_floor_type\" and \"severe_damage\"\n",
    "pd.pivot_table(df, index='ground_floor_type', \n",
    "               values='severe_damage', aggfunc='mean', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b4740-2a35-4d6f-9a14-acbbcf228b4d",
   "metadata": {},
   "source": [
    "#### Foundation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f166d2-88e5-4dbe-b968-c3ffa0d46746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the relationship between \"foundation_type\" and \"severe_damage\"\n",
    "pd.pivot_table(df, index='foundation_type', \n",
    "               values='severe_damage', aggfunc='mean', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8027a1-2fb3-41e3-b51b-04fc13bf1562",
   "metadata": {},
   "source": [
    "### Dataset partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8030f-590d-459c-86eb-00a1626a88fb",
   "metadata": {},
   "source": [
    "Partitioning the dataset into target and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb43ea-5174-4ded-847a-852577f21316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the target and feature columns\n",
    "target = 'severe_damage'\n",
    "y = df[target]\n",
    "\n",
    "# features\n",
    "X = df.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561443e1-24aa-4ab0-9e7c-0c8bea29bf17",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759cabc-80fb-4a36-90a4-16bd8387a9d9",
   "metadata": {},
   "source": [
    "I'll split the dataset into training, testing sets using the `train_test_split` method in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c5ca2-2ffc-4cfc-91b8-275d7a33625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a71d5a-9a51-447d-9c22-149e58ff8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 20% of the training set as validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09675f-9eda-443a-b7ff-a244fd4d9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of training set\n",
    "print(f'Shape of X_train:{X_train.shape}')\n",
    "print(f'Shape of y_train:{y_train.shape}\\n')\n",
    "\n",
    "# Shape of validation set\n",
    "print(f'Shape of X_val:{X_val.shape}')\n",
    "print(f'Shape of y_val:{y_val.shape}\\n')\n",
    "\n",
    "# Shape of testing set\n",
    "print(f'Shape of X_test:{X_test.shape}')\n",
    "print(f'Shape of y_test:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6397a-a301-48a7-af84-8714d232f175",
   "metadata": {},
   "source": [
    "### Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9435b7-af21-4f86-bb38-0415dd4274de",
   "metadata": {},
   "source": [
    "#### Baseline Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176c718-dc24-4296-a63b-48cb64489ce4",
   "metadata": {},
   "source": [
    "Calculating the basline accuracy score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768db87-d4c6-46dc-ac3b-cfe52731b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline\n",
    "baseline_acc = y_train.value_counts(normalize=True).max()\n",
    "print(f'The baseline accuracy score is: {round(baseline_acc, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db691064-711e-4008-9820-bf5d6c8d0491",
   "metadata": {},
   "source": [
    "### Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572b8bc-46c3-4e49-956f-5d62163c43c3",
   "metadata": {},
   "source": [
    "In order to choose the most appropriate hyperparameters for the models max_depth, I need to iterate over a range of possible depths and choose the depth that produces the best accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ecd86-001f-4295-a432-d7188b1bfaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classification model\n",
    "\n",
    "depth_hyperparameters = range(1, 16)\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for d in depth:\n",
    "    tune_model = make_pipeline(OrdinalEncoder(), \n",
    "                          DecisionTreeClassifier(random_state=42, max_depth=d))\n",
    "\n",
    "    # Fit model with training data\n",
    "    tune_model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate training accuracy score\n",
    "    train_acc = tune_model.score(X_train, y_train)\n",
    "    train_acc_list.append(round(train_acc, 8))\n",
    "\n",
    "    # Calculate the accuracy score of the validation set\n",
    "    val_acc = tune_model.score(X_val, y_val)\n",
    "    val_acc_list.append(round(val_acc, 8))\n",
    "    \n",
    "print(train_acc_list)\n",
    "print(val_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c35af-e2b5-47ef-a16c-ea4e481e2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = pd.Series(val_acc_list, index=depth)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5eb31-9507-489d-827d-92a06a74b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_list, label='training')\n",
    "plt.plot(val_acc_list, label='validation')\n",
    "plt.title('Validation Curve: Decision Tree Model')\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590a498-9f72-4f59-8d52-b31d06444b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with best `max_depth`\n",
    "model = make_pipeline(OrdinalEncoder(),\n",
    "                     DecisionTreeClassifier(random_state=42, max_depth=10))\n",
    "\n",
    "# fit the model with the training dataset\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf17c7f-cad7-4225-bda3-841adf845842",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c89881-7511-4c89-82b5-63c6bd18e888",
   "metadata": {},
   "source": [
    "To see how the model performs with the training set compared to the baseline accuracy score, I calculated the accuracy score of the model with test data and compare it to the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64f0e9-0c91-417a-acbc-718921edf056",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = model.score(X_test, y_test)\n",
    "print(f'Accuracy score with Test data: {acc_test}')\n",
    "print(f'Baseline accuracy score: {baseline_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ef0d6-c919-4494-b765-919bf6013cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c281007-d81e-43f7-b40b-92cb96e5b0b6",
   "metadata": {},
   "source": [
    "From the result, the model beats the baseline, hence it is has good performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1a4ed-960d-41aa-a452-7a484a056298",
   "metadata": {},
   "source": [
    "### Communicating result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8e241-3b49-49ac-aa95-1706abe0ebdf",
   "metadata": {},
   "source": [
    "To communicate the result in the best way, I'll use features and importances to create a plot that exposes the most important and least important features that determines the severity of the earthquakes in Sindhupalchok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ab167-5c28-49f4-a84d-acef3c4f05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feautures\n",
    "features = X_train.columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec363cb7-5d6b-4fcc-bbbd-2d79a7113ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = model.named_steps['decisiontreeclassifier'].feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909eca2-5c08-4db3-bdd0-818694c4ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(importances, index=features).sort_values()\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb5533-7b9a-4d64-affc-48d68ef32296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart \n",
    "feature_imp.plot(kind='barh')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance: Sindhupalchok Decision Tree');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43195f9-7202-43ea-99e0-c069148ed83a",
   "metadata": {},
   "source": [
    "#### Tree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9d80a-5063-48bb-8b4d-0a3b1c121fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 15))\n",
    "plot_tree(\n",
    "    decision_tree=model.named_steps['decisiontreeclassifier'],\n",
    "    feature_names= features,\n",
    "    max_depth= 2,\n",
    "    filled=True,\n",
    "    rounded=False,\n",
    "    precision=4,\n",
    "    ax=ax\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005ccca-8008-4e36-83c1-4592d24c3f5f",
   "metadata": {},
   "source": [
    "#### Damage Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f8152-35b2-4cf1-be95-54e2e6afe3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make prediction\n",
    "def predict_building_damage(age_building, plinth_area_sq_ft, height_ft_pre_eq, \n",
    "                          land_surface_condition, foundation_type, roof_type,\n",
    "                          ground_floor_type, other_floor_type, position, \n",
    "                          plan_configuration, superstructure\n",
    "                         ):\n",
    "    data = {\n",
    "        'age_building': age_building,\n",
    "        'plinth_area_sq_ft': plinth_area_sq_ft,\n",
    "        'height_ft_pre_eq': height_ft_pre_eq,\n",
    "        'land_surface_condition': land_surface_condition,\n",
    "        'foundation_type': foundation_type,\n",
    "        'roof_type': roof_type,\n",
    "        'ground_floor_type': ground_floor_type,\n",
    "        'other_floor_type': other_floor_type,\n",
    "        'position': position,\n",
    "        'plan_configuration': plan_configuration,\n",
    "        'superstructure': superstructure\n",
    "    }\n",
    "    \n",
    "    X = pd.DataFrame(data, index=[0])\n",
    "    \n",
    "    severe_damage = model.predict(X)[0]\n",
    "\n",
    "    if severe_damage == 1:\n",
    "        return \"Your Building will most likely suffer severe damage if an earthquake occurs\"\n",
    "    else:\n",
    "        return 'Your building is not at risk of suffering any severe damage by an earthquake'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edb959-33ff-4c0f-b345-11ab3ed95d82",
   "metadata": {},
   "source": [
    "### Communicating Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1352c8-fd35-47ed-af93-2a8c33eebf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = interact(\n",
    "            predict_building_damage,\n",
    "            age_building=IntSlider(\n",
    "                value= X_train['age_building'].mean(),\n",
    "                min=X_train['age_building'].min(),\n",
    "                max=X_train['age_building'].max()\n",
    "            ),\n",
    "            \n",
    "            plinth_area_sq_ft=IntSlider(\n",
    "                value=X_train['plinth_area_sq_ft'].mean(),\n",
    "                min=X_train['plinth_area_sq_ft'].min(),\n",
    "                max=X_train['plinth_area_sq_ft'].max()\n",
    "            ),\n",
    "        \n",
    "            height_ft_pre_eq=IntSlider(\n",
    "                value=X_train['height_ft_pre_eq'].mean(),\n",
    "                min=X_train['height_ft_pre_eq'].min(),\n",
    "                max=X_train['height_ft_pre_eq'].max()\n",
    "            ),\n",
    "        \n",
    "            land_surface_condition=Dropdown(\n",
    "                options=X_train['land_surface_condition'].unique().tolist(),\n",
    "                description='Land Surface Condition',\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "            foundation_type=Dropdown(\n",
    "                options=X_train['foundation_type'].unique().tolist(),\n",
    "                description='Foundation Type',\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "        \n",
    "            roof_type=Dropdown(\n",
    "                options=X_train['roof_type'].unique().tolist(),\n",
    "                description='Roof type',\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "            ground_floor_type=Dropdown(\n",
    "                options=X_train['ground_floor_type'].unique().tolist(),\n",
    "                description='Ground floor type',\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "            other_floor_type=Dropdown(\n",
    "                options=X_train['other_floor_type'].unique().tolist(),\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "                position=Dropdown(\n",
    "                options=X_train['position'].unique().tolist(),\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "            plan_configuration=Dropdown(\n",
    "                options=X_train['plan_configuration'].unique().tolist(),\n",
    "                description='Plan Configuration',\n",
    "                disabled=False\n",
    "            ),\n",
    "        \n",
    "            superstructure=Dropdown(\n",
    "                options=X_train['superstructure'].unique().tolist(),\n",
    "                description='Superstructure',\n",
    "                disabled=False\n",
    "            )\n",
    ")\n",
    "display(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903500eb-4f31-4003-b9c7-773cb3fffa7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
